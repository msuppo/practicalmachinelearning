---
title: "Prediction Assignment Writeup"
author: "msuppo"
date: "Monday, April 25, 2016"
output: html_document
---

##Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, we will use data recorded from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

More information is available from the website http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The goal of this project is to predict the manner in which the volunteers did the exercise. The outcome is measured against 60 observations and classifed as "A,B,C,D,E" catogerise and it is stored in the classe variable in the data set.

This assignment will: 

1. create a report describing how the model is built
2. how cross validation is use
3. what is the expected out of sample error and why the choices were made
4. use prediction model to predict 20 different test cases 

##Data Loading and Cleaning 

The data for this assignment was obtained from http://groupware.les.inf.puc-rio.br/har  
Two data set were available: training data and testing data. 

Excluding variables with at least one "NA" and variables with nearly zero variance. Predictor Candidates: Belt, arm, dumbbell and forearm variables do not have any missing values in the test dataset. Then, Subset the Training dataset to include only the predictor candidates and the outcome variable: classe. Make classe into a factor. Split the dataset into a 60% training and 40% for cross-validation dataset.


```{r,  message=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
require(data.table)
```
```{r}
setwd("C:/Users/usuariodell/Desktop/DS/R ejercicios")

setInternet2(TRUE)
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
Data <- fread(urlTrain)
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
DataTest <- fread(urlTest)
hayMissing <- sapply(DataTest, function (x) any(is.na(x) | x == ""))
esPredictor <- !hayMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(hayMissing))
pCandidatos <- names(hayMissing)[esPredictor]
pCandidatos

varCandidatos <- c("classe", pCandidatos)
Data <- Data[, varCandidatos, with=FALSE]
dim(Data)

Data <- Data[, classe := factor(Data[, classe])]
Data[, .N, classe]

plot(Data$classe, col="salmon", main="Bar Plot: Levels of Training dataset - classe variable", xlab="Classe", ylab="Frequency")
```

## Splitting the Data (training and testing) for cross validation.

The resulted Data is Partitioned to obtain a 70% training set and a 30% test set, in order to generate a test set for cross validation independent from the 20 cases provided set. Preprocessing the prediction variables by centering and scaling. Then, applying the centering and scaling to the testing dataset.

```{r}

set.seed(16369)
Training <- createDataPartition(Data$classe, p=0.7)
DatosTrain <- Data[Training[[1]]]
DatosProbe <- Data[-Training[[1]]]
Pto <- DatosTrain[, pCandidatos, with=FALSE]
preProc <- preProcess(Pto)
PtoN <- predict(preProc, Pto)
DatosTrainN <- data.table(data.frame(classe = DatosTrain[, classe], PtoN))
Pto <- DatosProbe[, pCandidatos, with=FALSE]
PtoN <- predict(preProc, Pto)
DatosProbeN <- data.table(data.frame(classe = DatosProbe[, classe], PtoN))

```

##Fitting and Training the Predicion Model

Method used: Random Forest. Random forests build lots of bushy trees, and then average them to reduce the variance. I choose this method because normally have a very hight accuracy. Using this method, the out of sample error should be very small (3% or less). If that is the case, Random Forest would be a very good option for Prediction. The error will be estimated using the 30% testing sample. 


```{r, cache=TRUE}
methodo <- "rf"
if (file.exists("TModelo.RData")) {
  load("TModelo.RData") 
  } else 
    TModelo <- train(classe ~ ., data=DatosTrainN, method=methodo)
```

##Evaluating the model on the Training dataset
```{r}
TModelo
h <- predict(TModelo, DatosTrainN)
confusionMatrix(h, DatosTrain[, classe])
```
Fitting the Model with the Training data set and using Random Forest, the accuracy was 100%. There are two options: it is a really good model or it was over-fit. In the next section, the model will be evaluate using the cross-validation dataset to check those hipotesis.

##Evaluating the model on the Cross-Validation dataset
Now, the model will be evaluate using the 30% testing sample.
```{r}
h <- predict(TModelo, DatosProbeN)
confusionMatrix(h, DatosProbeN[, classe])
```
Fitting the Model with the Cross-Validation dataset in this model that uses Random Forest, the accuracy was 99.12%. So, the hipotesis about this model worked very well was validated.

##Showing the Final model 
```{r}
varImp(TModelo)
TModelo$finalModel

save(TModelo, file="TModelo.RData")
varImpPlot(TModelo$finalModel, type=2, main= "Dotchart: Variable importance by a Random Forest", col="salmon", cex=0.7)
```

The OOB estimated error rate of the Final Model is 0.8%.


##Expected out-of-sample error

The expected out-of-sample error is estimated at 0.008, or 0.8%. 
The expected out-of-sample error is calculated: 1 - accuracy, for predictions made against the cross-validation set. The Test data set has 20 cases. With an accuracy the moren than 99% on the cross-validation data, it can be expected that very few, or none, of the test samples will be missclassified.


##Predicting on the Test Data
```{r}
load(file="TModelo.RData", verbose=TRUE)

DataTestN <- predict(preProc, DataTest[, pCandidatos, with=FALSE])
FinalTest <- predict(TModelo, DataTestN)
FinalTest
```

